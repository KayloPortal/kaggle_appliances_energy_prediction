{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08564a27",
   "metadata": {},
   "source": [
    "## Experminet Notebook\n",
    "This notebook includes the process of training multiple models, monitoring performance, comparison and model selection over the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ceca9d",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e1e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db010972",
   "metadata": {},
   "source": [
    "## Initiliazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e2cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffb65ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11 17:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-11 17:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-11 17:20:00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-11 17:30:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 17:40:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>2016-05-27 17:20:00</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>2016-05-27 17:30:00</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>2016-05-27 17:40:00</td>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>2016-05-27 17:50:00</td>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>2016-05-27 18:00:00</td>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  Appliances  lights         T1       RH_1  \\\n",
       "0      2016-01-11 17:00:00          60      30  19.890000  47.596667   \n",
       "1      2016-01-11 17:10:00          60      30  19.890000  46.693333   \n",
       "2      2016-01-11 17:20:00          50      30  19.890000  46.300000   \n",
       "3      2016-01-11 17:30:00          50      40  19.890000  46.066667   \n",
       "4      2016-01-11 17:40:00          60      40  19.890000  46.333333   \n",
       "...                    ...         ...     ...        ...        ...   \n",
       "19730  2016-05-27 17:20:00         100       0  25.566667  46.560000   \n",
       "19731  2016-05-27 17:30:00          90       0  25.500000  46.500000   \n",
       "19732  2016-05-27 17:40:00         270      10  25.500000  46.596667   \n",
       "19733  2016-05-27 17:50:00         420      10  25.500000  46.990000   \n",
       "19734  2016-05-27 18:00:00         430      10  25.500000  46.600000   \n",
       "\n",
       "              T2       RH_2         T3       RH_3         T4  ...         T9  \\\n",
       "0      19.200000  44.790000  19.790000  44.730000  19.000000  ...  17.033333   \n",
       "1      19.200000  44.722500  19.790000  44.790000  19.000000  ...  17.066667   \n",
       "2      19.200000  44.626667  19.790000  44.933333  18.926667  ...  17.000000   \n",
       "3      19.200000  44.590000  19.790000  45.000000  18.890000  ...  17.000000   \n",
       "4      19.200000  44.530000  19.790000  45.000000  18.890000  ...  17.000000   \n",
       "...          ...        ...        ...        ...        ...  ...        ...   \n",
       "19730  25.890000  42.025714  27.200000  41.163333  24.700000  ...  23.200000   \n",
       "19731  25.754000  42.080000  27.133333  41.223333  24.700000  ...  23.200000   \n",
       "19732  25.628571  42.768571  27.050000  41.690000  24.700000  ...  23.200000   \n",
       "19733  25.414000  43.036000  26.890000  41.290000  24.700000  ...  23.200000   \n",
       "19734  25.264286  42.971429  26.823333  41.156667  24.700000  ...  23.200000   \n",
       "\n",
       "          RH_9      T_out  Press_mm_hg     RH_out  Windspeed  Visibility  \\\n",
       "0      45.5300   6.600000        733.5  92.000000   7.000000   63.000000   \n",
       "1      45.5600   6.483333        733.6  92.000000   6.666667   59.166667   \n",
       "2      45.5000   6.366667        733.7  92.000000   6.333333   55.333333   \n",
       "3      45.4000   6.250000        733.8  92.000000   6.000000   51.500000   \n",
       "4      45.4000   6.133333        733.9  92.000000   5.666667   47.666667   \n",
       "...        ...        ...          ...        ...        ...         ...   \n",
       "19730  46.7900  22.733333        755.2  55.666667   3.333333   23.666667   \n",
       "19731  46.7900  22.600000        755.2  56.000000   3.500000   24.500000   \n",
       "19732  46.7900  22.466667        755.2  56.333333   3.666667   25.333333   \n",
       "19733  46.8175  22.333333        755.2  56.666667   3.833333   26.166667   \n",
       "19734  46.8450  22.200000        755.2  57.000000   4.000000   27.000000   \n",
       "\n",
       "       Tdewpoint        rv1        rv2  \n",
       "0       5.300000  13.275433  13.275433  \n",
       "1       5.200000  18.606195  18.606195  \n",
       "2       5.100000  28.642668  28.642668  \n",
       "3       5.000000  45.410389  45.410389  \n",
       "4       4.900000  10.084097  10.084097  \n",
       "...          ...        ...        ...  \n",
       "19730  13.333333  43.096812  43.096812  \n",
       "19731  13.300000  49.282940  49.282940  \n",
       "19732  13.266667  29.199117  29.199117  \n",
       "19733  13.233333   6.322784   6.322784  \n",
       "19734  13.200000  34.118851  34.118851  \n",
       "\n",
       "[19735 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"./KAG_energydata_complete.csv\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742a86b",
   "metadata": {},
   "source": [
    "## Research Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9870756",
   "metadata": {},
   "source": [
    "A class to train models with different specifications in just one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c08ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCreator:\n",
    "  def __init__(self, weekday_enc='ohe', hour_enc='ohe', reg_type='lasso', poly_deg=1, random_state=None, trainer=None, split_rate=0.2):\n",
    "    self.params = {\n",
    "      'weekday_enc': weekday_enc,\n",
    "      'hour_enc': hour_enc,\n",
    "      'reg_type': reg_type,\n",
    "      'poly_deg': poly_deg,\n",
    "      'random_state': random_state,\n",
    "      'trainer': trainer,\n",
    "      'split_rate': split_rate\n",
    "    }\n",
    "    # self.model = None\n",
    "    # self.results = {}\n",
    "    self.encoders = {\n",
    "      'hour': None,\n",
    "      'weekday': None\n",
    "    }\n",
    "    self.ohe_counts = None\n",
    "    self.poly = None\n",
    "    \n",
    "  def __cyclic_encode(self, data, max_val):\n",
    "    return np.column_stack([\n",
    "        np.sin(2 * np.pi * data / max_val),\n",
    "        np.cos(2 * np.pi * data / max_val)\n",
    "    ])\n",
    "\n",
    "  def __ohe_encode(self, data, key):\n",
    "    if self.encoders[key] == None:\n",
    "      encoder_data = OneHotEncoder(sparse_output=False)\n",
    "      self.encoders[key] = encoder_data\n",
    "      encoder_data.fit(data.reshape(-1, 1))\n",
    "      # return (encoder_data.fit_transform(data.reshape(-1, 1)), encoder_data)\n",
    "    return (self.encoders[key].transform(data.reshape(-1, 1)), self.encoders[key])\n",
    "      \n",
    "\n",
    "  def __binned_encode(self, data):\n",
    "    pass\n",
    "    \n",
    "  def _process_dataframe(self, df):\n",
    "    # --- Handle The Dataframe ---\n",
    "    dataframe = df.copy()\n",
    "    date_column = pd.to_datetime(dataframe['date'])\n",
    "    hours = date_column.dt.hour.values # extracts hour column as [0, 0, ..., 2, 2, ...]\n",
    "    weekdays = date_column.dt.weekday.values # extracting weekdays like [0, 0, ..., 1, 1, ...]\n",
    "    dataframe = dataframe.drop('date', axis=1)\n",
    "    \n",
    "    # --- Handle Hour Encoding ---\n",
    "    if self.params['hour_enc'] == 'ohe':\n",
    "      hour_encoded, encoder_hour = self.__ohe_encode(hours, 'hour')\n",
    "      hour_column_names = encoder_hour.get_feature_names_out(['hour'])\n",
    "      self.encoder_hour = encoder_hour\n",
    "    elif self.params['hour_enc'] == 'trig':\n",
    "      hour_encoded = self.__cyclic_encode(hours, 24)\n",
    "      hour_column_names = np.array(['hour_sin', 'hour_cos'])\n",
    "    elif self.params['hour_enc'] == 'binned':\n",
    "      pass\n",
    "    else: raise Exception(\"this hour_enc is not supported or is not written correctly, please double check. supported hour_enc values: ohe, trig, binned\")\n",
    "    # put the hour-of-day into dataframe\n",
    "    hour_encoded_df = pd.DataFrame(hour_encoded, columns=hour_column_names)\n",
    "    dataframe = pd.concat([hour_encoded_df, dataframe], axis=1)\n",
    "    \n",
    "    # --- Handle Weekday Encoding ---   \n",
    "    if self.params['weekday_enc'] == 'ohe':\n",
    "      weekdays_encoded, encoder_weekdays = self.__ohe_encode(weekdays, 'weekday')\n",
    "      weekdays_column_names = encoder_weekdays.get_feature_names_out(['weekday'])\n",
    "    elif self.params['weekday_enc'] == 'trig':\n",
    "      weekdays_encoded = self.__cyclic_encode(weekdays, 7)\n",
    "      weekdays_column_names = np.array(['weekday_sin', 'weekday_cos'])\n",
    "    elif self.params['weekday_enc'] == 'binned':\n",
    "      pass\n",
    "    # Putting weekday columns into dataframe\n",
    "    week_encoded_df = pd.DataFrame(weekdays_encoded, columns=weekdays_column_names)\n",
    "    dataframe = pd.concat([week_encoded_df, dataframe], axis=1)\n",
    "    \n",
    "    # --- Handle Polynomial Features ---\n",
    "    if 'Appliances' in df:\n",
    "      X = dataframe.drop('Appliances', axis=1)\n",
    "      y = dataframe[['Appliances']]\n",
    "    else:\n",
    "      X = dataframe\n",
    "      y = None\n",
    "    \n",
    "    if self.params['poly_deg'] > 1:\n",
    "      if self.poly == None:\n",
    "        poly = PolynomialFeatures(degree=self.params['poly_deg'], include_bias=False)\n",
    "        X = poly.fit_transform(X)\n",
    "        self.poly = poly\n",
    "      else:\n",
    "        poly = self.poly\n",
    "        X = poly.transform(X)\n",
    "      # Filtering out bad ohe combinations\n",
    "      if self.ohe_counts == None:\n",
    "        ohe_counts = []\n",
    "        if self.params['weekday_enc'] == 'ohe':\n",
    "          if self.params['hour_enc'] == 'ohe':\n",
    "            ohe_counts = [(0, 7), (7, 7 + 24)]\n",
    "          elif self.params['hour_enc'] == 'binned':\n",
    "            pass\n",
    "          elif self.params['hour_enc'] == 'trig':\n",
    "            ohe_counts = [(0, 7)]\n",
    "        if self.params['weekday_enc'] == 'trig':\n",
    "          if self.params['hour_enc'] == 'ohe':\n",
    "            ohe_counts = [(2, 2 + 24)]\n",
    "          elif self.params['hour_enc'] == 'binned':\n",
    "            pass\n",
    "        if self.params['weekday_enc'] == 'binned':\n",
    "          if self.params['hour_enc'] == 'ohe':\n",
    "            pass\n",
    "          elif self.params['hour_enc'] == 'binned':\n",
    "            pass\n",
    "          elif self.params['hour_enc'] == 'trig':\n",
    "            pass\n",
    "        self.ohe_counts = ohe_counts\n",
    "        self.valid = []\n",
    "        if len(ohe_counts) > 0:\n",
    "          feature_powers = self.poly.powers_\n",
    "          for i in range(len(feature_powers)):\n",
    "            # if (feature_powers[i, ohe_counts[0][0]:ohe_counts[0][1]].sum() <= 1\n",
    "            #     and feature_powers[i, ohe_counts[1][0]:ohe_counts[1][1]].sum() <= 1): self.valid.append(i)\n",
    "            isValid = True\n",
    "            for item in ohe_counts:\n",
    "              if feature_powers[i, item[0]:item[1]].sum() > 1: isValid = False\n",
    "            if isValid: self.valid.append(i)\n",
    "      \n",
    "      if len(self.ohe_counts) > 0:\n",
    "        X = X[:, self.valid]\n",
    "    return (X, y)      \n",
    "\n",
    "  # def _extract_weekday(self, date_string: str) -> int:\n",
    "  #   return datetime.strptime(date_string, \"%Y-%m-%d\").weekday()\n",
    "\n",
    "  def fit(self, df):\n",
    "    \"\"\"Constructs the pipeline and trains it.\"\"\"\n",
    "    \n",
    "    X, y = self._process_dataframe(df)\n",
    "\n",
    "    # --- Split ---\n",
    "    if self.params['split_rate'] == 0: \n",
    "      # This means we don't want to split our data, we want the model to be trained on the whole dataframe.\n",
    "      # ATTENTION: You're allowed to use self.evaluate only when you've set the split_rate a positive number.\n",
    "      X_train = X\n",
    "      y_train = y\n",
    "    else:\n",
    "      # This means we want a test subset to be extracted from our dataframe\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.params['split_rate'], random_state=self.params['random_state'])\n",
    "      y_test = y_test.values.squeeze()\n",
    "      self.X_test = X_test\n",
    "      self.y_test = y_test\n",
    "    y_train = y_train.values.squeeze()\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "    \n",
    "    # --- PipeLine: Feature scaling & Training ---\n",
    "    if self.params['reg_type'] == 'lasso':\n",
    "      if self.params['trainer'] == None:\n",
    "        self.params['trainer'] = LassoCV(cv=5, random_state=self.params['random_state'], n_jobs=-1, max_iter=10000)\n",
    "      self.pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso', self.params['trainer'])\n",
    "      ])\n",
    "    elif self.params['reg_type'] == 'ridge':\n",
    "      if self.params['trainer'] == None:\n",
    "        self.params['trainer'] = RidgeCV()\n",
    "      self.pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', self.params['trainer'])        \n",
    "      ])\n",
    "    elif self.params['reg_type'] == 'regressor':\n",
    "      if self.params['trainer'] == None:\n",
    "        self.params['trainer'] = LinearRegression()\n",
    "      self.pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', self.params['trainer'])        \n",
    "      ])\n",
    "    else: raise Exception(\"this reg_type is not supported or is not written correctly, please double check. supported reg_type values: lasso, ridge, regressor\")\n",
    "  \n",
    "    self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, df, return_labels=False):\n",
    "    \"\"\"Gets raw dataframe and outputs model's predictions. Dataframe's structure should be exactly like the original dataframe, with the exception that including Appliance column is optional\"\"\"\n",
    "    X, y = self._process_dataframe(df)\n",
    "    y_pred = self.pipeline.predict(X)\n",
    "    if return_labels: return (y_pred, y)\n",
    "    return y_pred\n",
    "    \n",
    "  \n",
    "  # def evaluate(self, X_test, y_test):\n",
    "    # \"\"\"Calculates metrics and stores them in self.results.\"\"\"\n",
    "    # y_pred = self.model.predict(X_test)\n",
    "    # self.results['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    # self.results['r2'] = r2_score(y_test, y_pred)\n",
    "    # return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5da804",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd942c9",
   "metadata": {},
   "source": [
    "### How to Use The Engine: Training a Basic Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c20887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model = ModelCreator(hour_enc='ohe', weekday_enc='ohe', poly_deg=1, reg_type='regressor', random_state=42\n",
    "#                             , trainer=LinearRegression(random_state_24)) # Declare specifications\n",
    "# linear_model.fit(dataframe) # Train the model\n",
    "# y_pred = linear_model.pipeline.predict(linear_model.X_test) # Raise answers\n",
    "# RMSE = np.sqrt(mean_squared_error(linear_model.y_test, y_pred)) # calculate error\n",
    "# RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84ea95",
   "metadata": {},
   "source": [
    "### Choosing The Proper Encoding For Linear(degree=1) Models\n",
    "\n",
    "Four models were trained differing only in encoding methods. As expected, the model with one-hot encoding for both variables wins in generalization(due to ratios being closer to one), it also has the highest ability of explaining the variance of the data(Highest $R^2$) along with lowest average prediction inaccuracy(lowest RMSE on both train and test). However, it's obvious that all these models are underfitting and giving us garbage results, we can see that by comaring MAE(represents average distance between the model's predicted value and actual value) **MAE=50.88(Wh)** to average(mean) energy consumption in our data **mean=97.69(Wh)**. It's clear that the model is so innaccurate in guessing our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e1391",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 50.88 |\n",
    "| 1 | ohe | trig | lasso | 0.232 | 0.228 | 90.37 | 87.92 | 0.973 | 50.85 |\n",
    "| 1 | trig | ohe | lasso | 0.203 | 0.205 | 92.08 | 89.19 | 0.969 | 51.68 |\n",
    "| 1 | trig | trig | lasso | 0.200 | 0.201 | 92.26 | 89.40 | 0.969 | 51.62 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b20586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 50.88 |\n",
      "| 1 | ohe | trig | lasso | 0.232 | 0.228 | 90.37 | 87.92 | 0.973 | 50.85 |\n",
      "| 1 | trig | ohe | lasso | 0.203 | 0.205 | 92.08 | 89.19 | 0.969 | 51.68 |\n",
      "| 1 | trig | trig | lasso | 0.200 | 0.201 | 92.26 | 89.40 | 0.969 | 51.62 |\n"
     ]
    }
   ],
   "source": [
    "# Define your experimental grid\n",
    "experiments = [\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "]\n",
    "\n",
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE |\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "\n",
    "for params in experiments:\n",
    "    # 1. Initialize and Fit\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe) # Assuming your raw data is in 'dataframe'\n",
    "    \n",
    "    # 2. Get Metrics\n",
    "    # (Using the evaluate method logic we discussed)\n",
    "    y_train_pred = model.pipeline.predict(model.X_train)\n",
    "    y_test_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    tr_r2 = r2_score(model.y_train, y_train_pred)\n",
    "    ts_r2 = r2_score(model.y_test, y_test_pred)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(model.y_train, y_train_pred))\n",
    "    ts_rmse = np.sqrt(mean_squared_error(model.y_test, y_test_pred))\n",
    "    \n",
    "    mae = mean_absolute_error(model.y_test, y_test_pred)\n",
    "    \n",
    "    rmse_ratio = ts_rmse / tr_rmse\n",
    "    \n",
    "    # 3. Print Markdown Row\n",
    "    print(f\"| {params['poly_deg']} | {params['hour_enc']} | {params['weekday_enc']} | {params['reg_type']} | \"\n",
    "          f\"{tr_r2:.3f} | {ts_r2:.3f} | {tr_rmse:.2f} | {ts_rmse:.2f} | {rmse_ratio:.3f} | {mae:.2f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114a380",
   "metadata": {},
   "source": [
    "### Choosing The Proper Regularization Type For Linear(degree=1) Models\n",
    "We concluded from the previous cell that OHE encoding is the best for our linear models, three models with different regularization methods were tested and they all gave the same results. The reason to this is obvious, we talked earlier briefly in the ReadMe that $\\lambda$ tries to move the model in bias-variance tradeoff towards the sweetspot. Our linear model is underfitting, it has high bias and low variance, therefore the model tries to increase the model's variance to move it towards the sweetspot by decreasing $\\lambda$, but it reaches the model's boundries, meaning $\\lambda$ gets near zero and tries it best to increase the variance, but the model has reached its limits and it can't go further(can't have more variance). Hence, all models, regardless of their regularization method, reach the same spot.\n",
    "\n",
    "In another words, even when $\\lambda = 0$(normal regression without regularization) the model is underfitting, meaning it has high bias and what it needs is more variance, and any $\\lambda > 0$ provides less variance, hence, the model tries to push $\\lambda$ towards zero, pushing all regulrazied regressions towards becoming a normal regression without regularization, therefore all four models end up being the same(not excatly the same, but almost identical).\n",
    "\n",
    "Considering the observations above, we conclude that when our model is underfitting, there's nothing regularization L1 or L2 can do, but to give us the same model we had. Utilizing regularization methods like L1 and L2 when the model is underfitting doesn't provide any value.\n",
    "\n",
    "Regularization is meant to reduce model's complexity to decrease variance, using it when the model is too simple and needs more variance doesn't help, therefore, the best linear model with degree one is a regular linear regression model with one-hot encoded weekdays and hours but without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dd7c9",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Best $\\lambda$ | CrossVal |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 | 5 Folds |\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | LOOCV |\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 5 Folds |\n",
    "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a6236",
   "metadata": {},
   "source": [
    "But you might ask, if L1 and L2 try to push the $\\lambda$ to zero for this model, why does this happens for lasso($\\lambda = 0.0197$) but not for ridge($\\lambda = 11.4976$)? Even when we try to train a ridge model with alphas=[0.0197, 11.4976], we expect the model to pick the less lambda based on the logic we provided above to increase the model's variance, but it picks 11.4976(check the code below to see), does this mean our logic is wrong? Well, not percisely.\n",
    "\n",
    "While it seems that ridge is working against this logic, this is not actually true. I trained two ridge models, one with $\\lambda = 0.0197$ and another with $\\lambda = 11.4976$, and look these wonderful table, they both raised the same results! This means both $\\lambda$ values give our model almost the same variance, they both create the same model. But why?\n",
    "\n",
    "The reason comes from how Ridge treats our data. Lasso is much more aggressive in penalizing variance comparing to Ridge, hence it's much more sensitive to changing the value of $\\lambda$. As shown in the table below, the same change for $\\lambda$ effects Lasso regression remarkably while Ridge regression stays almost the same.\n",
    "\n",
    "But if ridge regression with $\\lambda = 0.0197$ and $\\lambda = 11.4976$ are almost identical, why doesn't the model choose the lower lambda? that lower lambda might have the same performance but just be a little bit better, but this amount is so small the model can't see it properly.\n",
    "\n",
    "Our data is noisy, and K-Fold cross validation isn't perfect. Lasso with $\\lambda = 0.0197$ might be $0.00001$ better in performance, but the noise and very little uncontrolable bias that can happen in K-Fold CV, can hide this fact, making the model think $\\lambda = 11.4976$ is an option that's a little bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a31476",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Fixed $\\lambda$ | CrossVal |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 0.0197 | 5 Folds |\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 5 Folds |\n",
    "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 | 5 Folds |\n",
    "| 1 | ohe | ohe | lasso | 0.056 | 0.060 | 100.22 | 97.01 | 0.968 | 11.4976 | 5 Folds |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2eeba",
   "metadata": {},
   "source": [
    "Here's a visual summary:\n",
    "\n",
    "<img src=\"./visuals/diagram-bias-variance-tradeoff.png\" alt=\"bias variance tradeoff explained with diagram\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ec3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Best $\\lambda$ |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 |\n",
      "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | -1.0000 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 0.0197 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 |\n",
      "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 |\n",
      "| 1 | ohe | ohe | lasso | 0.056 | 0.060 | 100.22 | 97.01 | 0.968 | 11.4976 |\n"
     ]
    }
   ],
   "source": [
    "# Define your experimental grid\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "experiments = [\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'regressor', 'trainer': LinearRegression()},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=[0.0197, 11.4976])},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=[0.0197])},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=[11.4976])},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, alphas=[0.0197], random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, alphas=[11.4976], random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "]\n",
    "\n",
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Best $\\\\lambda$ |\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "\n",
    "for params in experiments:\n",
    "    # 1. Initialize and Fit\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe) # Assuming your raw data is in 'dataframe'\n",
    "    \n",
    "    # 2. Get Metrics\n",
    "    # (Using the evaluate method logic we discussed)\n",
    "    y_train_pred = model.pipeline.predict(model.X_train)\n",
    "    y_test_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    tr_r2 = r2_score(model.y_train, y_train_pred)\n",
    "    ts_r2 = r2_score(model.y_test, y_test_pred)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(model.y_train, y_train_pred))\n",
    "    ts_rmse = np.sqrt(mean_squared_error(model.y_test, y_test_pred))\n",
    "    \n",
    "    rmse_ratio = ts_rmse / tr_rmse\n",
    "    \n",
    "    try:\n",
    "        alpha = model.pipeline.named_steps[params['reg_type']].alpha_\n",
    "    except AttributeError: alpha = -1\n",
    "    \n",
    "    # 3. Print Markdown Row\n",
    "    print(f\"| {params['poly_deg']} | {params['hour_enc']} | {params['weekday_enc']} | {params['reg_type']} | \"\n",
    "          f\"{tr_r2:.3f} | {ts_r2:.3f} | {tr_rmse:.2f} | {ts_rmse:.2f} | {rmse_ratio:.3f} | {alpha:.4f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272dc6e9",
   "metadata": {},
   "source": [
    "### Choosing The Best Polynomial Model(degree two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af0176",
   "metadata": {},
   "source": [
    "As we saw, linear models with degree one could not explain our data prorperly. They were underfitting, lacking variance. Therefore, we increase the complexity of our model by changing its polynomial degree to two, to get a model with higher variance that has more chance to learn our data, due to its flexiblity to learn new patterns and less bias on the patterns it already knows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e063a45",
   "metadata": {},
   "source": [
    "#### Tuning Problem: LassoCV\n",
    "When training a lasso regression with one-hot encoded columns, I got a 'ConvergenceWarning' from scikit-learn. The data had over 1400 features with high multicollinearity due to a lot of columns representing same information or representing information that can be extraced from other columns. These multicollinearities happen often with noisy datasets like the one we are working with right now.\n",
    "\n",
    "Becuase the data had a lot of redunant features, the loss function plot's shape had become nearly flat around the optimal point. Lasso takes steps in a path towards that optimal point, and when it gets close, the shape becomes flat meaning lasso will take smaller and smaller steps, when we get to a point that we have taken lots of steps but we haven't reached the optimal point yet. This was the reason why we got the 'ConvergenceWarning' warning.\n",
    "\n",
    "To fix this problem, the hyperparmeters tol and max_iter needed to be tuned properly. max_iter is the maximum number of steps our lasso model takes to reach the optimal point. And to know about tol, first we have to know what is $Duality Gap$.\n",
    "\n",
    "Computitions aren't perfect. Computer can not always raech the exact optimal spot, but it can get very close to it, by taking steps in a path towards the point and stopping somewhere near the optimal point. To determine when we should stop, we define duality gap. Duality gap is the difference of the theoretical possible minimum for our cost function and the current value of our cost function. The theoretical minimums is calculated using mathematic relations. We can think of duality gap as the distance is left to reach the optimal point. Hence, we define tol. We tell the model that if your distance from the optimal point(duality gap) is less than the value of tol, you are close enough(convergence has happened), you are good to stop computitions and consider the weights of the current spot as an answer.\n",
    "\n",
    "$$\\text{Duality Gap} = \\text{Current Cost (Primal)} - \\text{Theoretical Floor (Dual)}$$\n",
    "\n",
    "The problem of our model was that it would stop in a spot that was not close enough to the optimal spot($Duality Gap > tol$) because it had reached the maximum number of steps it could take(max_iter). This means that convergence didn't happen, resulting in a 'ConvergenceWarning'. The model had finished the training and all the process without error, but its answer is not accurate and 'good enough'. Therefore, by increasing the value of tol and max_iter, we can make sure convergunce happens.\n",
    "\n",
    "The cost function used in LassoCV is:\n",
    "\n",
    "$$J(w) = \\frac{1}{2n_{\\text{samples}}} \\|y - Xw\\|_2^2 + \\alpha \\|w\\|_1$$\n",
    "\n",
    "The second term is L1 norm, the first term is actually $\\frac{1}{2} MSE$. It's clear that $2 \\times J(w) > MSE$ and $J_{\\text{answer}}(w) - J_{\\text{theoretical minimum}}(w) < tol$ and $MAE < MSE$. This means the difference of our final $MAE$ and the smallest possible $MAE$ is guaranteed to be smaller than two times the value of our $tol$. This logic helps us to increase $tol$ with open eyes without making the model too much innaccurate.\n",
    "\n",
    "$MAE$ shows the average innaccuracy of our model in predicting the target variable. Our target variable has an average(mean) value of 50(Wh). Hence, if the average error of our model is 0.05(Wh) more than the best $MAE$, it's still good, hence $tol=0.1$ seems like a good number. The previous $tol=10^{-4}$ was an overkill for this dataset.\n",
    "\n",
    "This increase in $tol$ means the model can reach a spot with more distance from the optimal value than before, but still be considered as a convergunce. This makes the destination more accessible, increasing the probablity that convergunce happens in a fair amount of steps. But still, we have to increase max_iter to let the model be able to take more steps, again increasing convergunce probablity. $40000$ seems like a good number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5843947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model):\n",
    "    y_train_pred = model.pipeline.predict(model.X_train)\n",
    "    y_test_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    tr_r2 = r2_score(model.y_train, y_train_pred)\n",
    "    ts_r2 = r2_score(model.y_test, y_test_pred)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(model.y_train, y_train_pred))\n",
    "    ts_rmse = np.sqrt(mean_squared_error(model.y_test, y_test_pred))\n",
    "    \n",
    "    rmse_ratio = ts_rmse / tr_rmse\n",
    "    r2_ratio = ts_r2 / tr_r2\n",
    "    \n",
    "    try:\n",
    "        alpha = model.pipeline.named_steps[params['reg_type']].alpha_\n",
    "    except AttributeError: alpha = -1    \n",
    "    \n",
    "    print(f\"| {params['poly_deg']} | {params['hour_enc']} | {params['weekday_enc']} | {params['reg_type']} | \"\n",
    "          f\"{tr_r2:.3f} | {ts_r2:.3f} | {tr_rmse:.2f} | {ts_rmse:.2f} | {rmse_ratio:.3f} | {r2_ratio:.3f} | {alpha:.4f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d58425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n"
     ]
    }
   ],
   "source": [
    "# Define your experimental grid\n",
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "]\n",
    "lasso_ohe_ohe = ModelCreator(**experiments[0], random_state=42)\n",
    "lasso_ohe_ohe.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\lambda$ |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n"
     ]
    }
   ],
   "source": [
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\\\lambda$ |\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "print_metrics(lasso_ohe_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70a08a",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 2 | ohe | ohe | lasso | 0.392 | 0.333 | 80.45 | 81.70 | 1.016 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bdef94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2 | ohe | ohe | ridge | 0.473 | 0.347 | 74.86 | 80.84 | 1.080 | 0.733 | 0.0071 |\n",
      "| 2 | ohe | trig | ridge | 0.445 | 0.343 | 76.83 | 81.08 | 1.055 | 0.771 | 0.0040 |\n",
      "| 2 | trig | ohe | ridge | 0.403 | 0.331 | 79.69 | 81.80 | 1.027 | 0.822 | 0.0035 |\n",
      "| 2 | trig | trig | ridge | 0.374 | 0.324 | 81.60 | 82.24 | 1.008 | 0.867 | 0.0023 |\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)}\n",
    "]\n",
    "\n",
    "for params in experiments:\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe)\n",
    "    print_metrics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdbe80",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\lambda$ |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 2 | ohe | ohe | ridge | 0.473 | 0.347 | 74.86 | 80.84 | 1.080 | 0.733 | 0.0071 |\n",
    "| 2 | ohe | trig | ridge | 0.445 | 0.343 | 76.83 | 81.08 | 1.055 | 0.771 | 0.0040 |\n",
    "| 2 | trig | ohe | ridge | 0.403 | 0.331 | 79.69 | 81.80 | 1.027 | 0.822 | 0.0035 |\n",
    "| 2 | trig | trig | ridge | 0.374 | 0.324 | 81.60 | 82.24 | 1.008 | 0.867 | 0.0023 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d62a6820",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[32m      9\u001b[39m     model = ModelCreator(**params, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     print_metrics(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mModelCreator.fit\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    171\u001b[39m   \u001b[38;5;28mself\u001b[39m.pipeline = Pipeline([\n\u001b[32m    172\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m'\u001b[39m, StandardScaler()),\n\u001b[32m    173\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mregressor\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m'\u001b[39m])        \n\u001b[32m    174\u001b[39m   ])\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mthis reg_type is not supported or is not written correctly, please double check. supported reg_type values: lasso, ridge, regressor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:621\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    616\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    617\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    618\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    619\u001b[39m             all_params=params,\n\u001b[32m    620\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2783\u001b[39m, in \u001b[36mRidgeCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   2743\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2744\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, **params):\n\u001b[32m   2745\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit Ridge regression model with cv.\u001b[39;00m\n\u001b[32m   2746\u001b[39m \n\u001b[32m   2747\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2781\u001b[39m \u001b[33;03m    the validation score.\u001b[39;00m\n\u001b[32m   2782\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2783\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2540\u001b[39m, in \u001b[36m_BaseRidgeCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   2531\u001b[39m     estimator.set_fit_request(sample_weight=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2533\u001b[39m grid_search = GridSearchCV(\n\u001b[32m   2534\u001b[39m     estimator,\n\u001b[32m   2535\u001b[39m     parameters,\n\u001b[32m   2536\u001b[39m     cv=cv,\n\u001b[32m   2537\u001b[39m     scoring=scorer,\n\u001b[32m   2538\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2540\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2541\u001b[39m estimator = grid_search.best_estimator_\n\u001b[32m   2542\u001b[39m \u001b[38;5;28mself\u001b[39m.alpha_ = grid_search.best_estimator_.alpha\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1053\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1047\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1048\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1049\u001b[39m     )\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1057\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1612\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1611\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:999\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    995\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    996\u001b[39m         )\n\u001b[32m    997\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1022\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:184\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m                 this_warning_filter_dict[special_key] = this_value.pattern\n\u001b[32m    182\u001b[39m         warnings.filterwarnings(**this_warning_filter_dict, append=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:833\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    831\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    836\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    837\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1264\u001b[39m, in \u001b[36mRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1253\u001b[39m xp, _ = get_namespace(X, y, sample_weight)\n\u001b[32m   1254\u001b[39m X, y = validate_data(\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1256\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1262\u001b[39m     y_numeric=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1263\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1006\u001b[39m, in \u001b[36m_BaseRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1003\u001b[39m         \u001b[38;5;66;03m# for dense matrices or when intercept is set to 0\u001b[39;00m\n\u001b[32m   1004\u001b[39m         params = {}\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28mself\u001b[39m.coef_, \u001b[38;5;28mself\u001b[39m.n_iter_, \u001b[38;5;28mself\u001b[39m.solver_ = \u001b[43m_ridge_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_solver\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_intercept(X_offset, y_offset, X_scale)\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:764\u001b[39m, in \u001b[36m_ridge_regression\u001b[39m\u001b[34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, positive, random_state, return_n_iter, return_intercept, return_solver, X_scale, X_offset, check_input, fit_intercept)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    763\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m         coef = \u001b[43m_solve_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m linalg.LinAlgError:\n\u001b[32m    766\u001b[39m         \u001b[38;5;66;03m# use SVD solver if matrix is singular\u001b[39;00m\n\u001b[32m    767\u001b[39m         solver = \u001b[33m\"\u001b[39m\u001b[33msvd\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:221\u001b[39m, in \u001b[36m_solve_cholesky\u001b[39m\u001b[34m(X, y, alpha)\u001b[39m\n\u001b[32m    218\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    219\u001b[39m n_targets = y.shape[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m A = \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m Xy = safe_sparse_dot(X.T, y, dense_output=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    224\u001b[39m one_alpha = np.array_equal(alpha, \u001b[38;5;28mlen\u001b[39m(alpha) * [alpha[\u001b[32m0\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\extmath.py:230\u001b[39m, in \u001b[36msafe_sparse_dot\u001b[39m\u001b[34m(a, b, dense_output)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    227\u001b[39m     ret = a @ b\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43msparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m sparse.issparse(b)\n\u001b[32m    232\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[32m    233\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[33m\"\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    234\u001b[39m ):\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.toarray()\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\_lib\\_sparse.py:10\u001b[39m, in \u001b[36missparse\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSparseABC\u001b[39;00m(ABC):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34missparse\u001b[39m(x):\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \u001b[33;03m    False\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, SparseABC)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)}\n",
    "]\n",
    "\n",
    "for params in experiments:\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe)\n",
    "    print_metrics(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
