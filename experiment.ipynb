{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08564a27",
   "metadata": {},
   "source": [
    "## Experminet Notebook\n",
    "This notebook includes the process of training multiple models, monitoring performance, comparison and model selection over the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba7e10",
   "metadata": {},
   "source": [
    "## 0. Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db010972",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f3487",
   "metadata": {},
   "source": [
    "### Import Modules And Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e2cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8279b",
   "metadata": {},
   "source": [
    "If you're running on colab cpu but writing in vscode, run this once and comment the cell after success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2e2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O /content/KAG_energydata_complete.csv \"https://raw.githubusercontent.com/KayloPortal/kaggle_appliances_energy_prediction/refs/heads/master/KAG_energydata_complete.csv\"\n",
    "\n",
    "# import os\n",
    "# if os.path.exists('/content/KAG_energydata_complete.csv'):\n",
    "#     print(\"Success! File is ready.\")\n",
    "# else:\n",
    "#     print(\"Download failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2a405",
   "metadata": {},
   "source": [
    "and run this code everytime you boot the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f9843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-91e514ff-40b7-49aa-8bf6-01ef89265670\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11 17:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-11 17:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-11 17:20:00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-11 17:30:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 17:40:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>2016-05-27 17:20:00</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>2016-05-27 17:30:00</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>2016-05-27 17:40:00</td>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>2016-05-27 17:50:00</td>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>2016-05-27 18:00:00</td>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 29 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91e514ff-40b7-49aa-8bf6-01ef89265670')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-91e514ff-40b7-49aa-8bf6-01ef89265670 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-91e514ff-40b7-49aa-8bf6-01ef89265670');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                      date  Appliances  lights         T1       RH_1  \\\n",
       "0      2016-01-11 17:00:00          60      30  19.890000  47.596667   \n",
       "1      2016-01-11 17:10:00          60      30  19.890000  46.693333   \n",
       "2      2016-01-11 17:20:00          50      30  19.890000  46.300000   \n",
       "3      2016-01-11 17:30:00          50      40  19.890000  46.066667   \n",
       "4      2016-01-11 17:40:00          60      40  19.890000  46.333333   \n",
       "...                    ...         ...     ...        ...        ...   \n",
       "19730  2016-05-27 17:20:00         100       0  25.566667  46.560000   \n",
       "19731  2016-05-27 17:30:00          90       0  25.500000  46.500000   \n",
       "19732  2016-05-27 17:40:00         270      10  25.500000  46.596667   \n",
       "19733  2016-05-27 17:50:00         420      10  25.500000  46.990000   \n",
       "19734  2016-05-27 18:00:00         430      10  25.500000  46.600000   \n",
       "\n",
       "              T2       RH_2         T3       RH_3         T4  ...         T9  \\\n",
       "0      19.200000  44.790000  19.790000  44.730000  19.000000  ...  17.033333   \n",
       "1      19.200000  44.722500  19.790000  44.790000  19.000000  ...  17.066667   \n",
       "2      19.200000  44.626667  19.790000  44.933333  18.926667  ...  17.000000   \n",
       "3      19.200000  44.590000  19.790000  45.000000  18.890000  ...  17.000000   \n",
       "4      19.200000  44.530000  19.790000  45.000000  18.890000  ...  17.000000   \n",
       "...          ...        ...        ...        ...        ...  ...        ...   \n",
       "19730  25.890000  42.025714  27.200000  41.163333  24.700000  ...  23.200000   \n",
       "19731  25.754000  42.080000  27.133333  41.223333  24.700000  ...  23.200000   \n",
       "19732  25.628571  42.768571  27.050000  41.690000  24.700000  ...  23.200000   \n",
       "19733  25.414000  43.036000  26.890000  41.290000  24.700000  ...  23.200000   \n",
       "19734  25.264286  42.971429  26.823333  41.156667  24.700000  ...  23.200000   \n",
       "\n",
       "          RH_9      T_out  Press_mm_hg     RH_out  Windspeed  Visibility  \\\n",
       "0      45.5300   6.600000        733.5  92.000000   7.000000   63.000000   \n",
       "1      45.5600   6.483333        733.6  92.000000   6.666667   59.166667   \n",
       "2      45.5000   6.366667        733.7  92.000000   6.333333   55.333333   \n",
       "3      45.4000   6.250000        733.8  92.000000   6.000000   51.500000   \n",
       "4      45.4000   6.133333        733.9  92.000000   5.666667   47.666667   \n",
       "...        ...        ...          ...        ...        ...         ...   \n",
       "19730  46.7900  22.733333        755.2  55.666667   3.333333   23.666667   \n",
       "19731  46.7900  22.600000        755.2  56.000000   3.500000   24.500000   \n",
       "19732  46.7900  22.466667        755.2  56.333333   3.666667   25.333333   \n",
       "19733  46.8175  22.333333        755.2  56.666667   3.833333   26.166667   \n",
       "19734  46.8450  22.200000        755.2  57.000000   4.000000   27.000000   \n",
       "\n",
       "       Tdewpoint        rv1        rv2  \n",
       "0       5.300000  13.275433  13.275433  \n",
       "1       5.200000  18.606195  18.606195  \n",
       "2       5.100000  28.642668  28.642668  \n",
       "3       5.000000  45.410389  45.410389  \n",
       "4       4.900000  10.084097  10.084097  \n",
       "...          ...        ...        ...  \n",
       "19730  13.333333  43.096812  43.096812  \n",
       "19731  13.300000  49.282940  49.282940  \n",
       "19732  13.266667  29.199117  29.199117  \n",
       "19733  13.233333   6.322784   6.322784  \n",
       "19734  13.200000  34.118851  34.118851  \n",
       "\n",
       "[19735 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe = pd.read_csv(\"/content/KAG_energydata_complete.csv\")\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ab21c",
   "metadata": {},
   "source": [
    "otherwise, If you're running on your local machine, just run this code instead of the two cells above, to load the database each time you boot the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09b5c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11 17:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-11 17:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-11 17:20:00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-11 17:30:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 17:40:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>2016-05-27 17:20:00</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>2016-05-27 17:30:00</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>2016-05-27 17:40:00</td>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>2016-05-27 17:50:00</td>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>2016-05-27 18:00:00</td>\n",
       "      <td>430</td>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  Appliances  lights         T1       RH_1  \\\n",
       "0      2016-01-11 17:00:00          60      30  19.890000  47.596667   \n",
       "1      2016-01-11 17:10:00          60      30  19.890000  46.693333   \n",
       "2      2016-01-11 17:20:00          50      30  19.890000  46.300000   \n",
       "3      2016-01-11 17:30:00          50      40  19.890000  46.066667   \n",
       "4      2016-01-11 17:40:00          60      40  19.890000  46.333333   \n",
       "...                    ...         ...     ...        ...        ...   \n",
       "19730  2016-05-27 17:20:00         100       0  25.566667  46.560000   \n",
       "19731  2016-05-27 17:30:00          90       0  25.500000  46.500000   \n",
       "19732  2016-05-27 17:40:00         270      10  25.500000  46.596667   \n",
       "19733  2016-05-27 17:50:00         420      10  25.500000  46.990000   \n",
       "19734  2016-05-27 18:00:00         430      10  25.500000  46.600000   \n",
       "\n",
       "              T2       RH_2         T3       RH_3         T4  ...         T9  \\\n",
       "0      19.200000  44.790000  19.790000  44.730000  19.000000  ...  17.033333   \n",
       "1      19.200000  44.722500  19.790000  44.790000  19.000000  ...  17.066667   \n",
       "2      19.200000  44.626667  19.790000  44.933333  18.926667  ...  17.000000   \n",
       "3      19.200000  44.590000  19.790000  45.000000  18.890000  ...  17.000000   \n",
       "4      19.200000  44.530000  19.790000  45.000000  18.890000  ...  17.000000   \n",
       "...          ...        ...        ...        ...        ...  ...        ...   \n",
       "19730  25.890000  42.025714  27.200000  41.163333  24.700000  ...  23.200000   \n",
       "19731  25.754000  42.080000  27.133333  41.223333  24.700000  ...  23.200000   \n",
       "19732  25.628571  42.768571  27.050000  41.690000  24.700000  ...  23.200000   \n",
       "19733  25.414000  43.036000  26.890000  41.290000  24.700000  ...  23.200000   \n",
       "19734  25.264286  42.971429  26.823333  41.156667  24.700000  ...  23.200000   \n",
       "\n",
       "          RH_9      T_out  Press_mm_hg     RH_out  Windspeed  Visibility  \\\n",
       "0      45.5300   6.600000        733.5  92.000000   7.000000   63.000000   \n",
       "1      45.5600   6.483333        733.6  92.000000   6.666667   59.166667   \n",
       "2      45.5000   6.366667        733.7  92.000000   6.333333   55.333333   \n",
       "3      45.4000   6.250000        733.8  92.000000   6.000000   51.500000   \n",
       "4      45.4000   6.133333        733.9  92.000000   5.666667   47.666667   \n",
       "...        ...        ...          ...        ...        ...         ...   \n",
       "19730  46.7900  22.733333        755.2  55.666667   3.333333   23.666667   \n",
       "19731  46.7900  22.600000        755.2  56.000000   3.500000   24.500000   \n",
       "19732  46.7900  22.466667        755.2  56.333333   3.666667   25.333333   \n",
       "19733  46.8175  22.333333        755.2  56.666667   3.833333   26.166667   \n",
       "19734  46.8450  22.200000        755.2  57.000000   4.000000   27.000000   \n",
       "\n",
       "       Tdewpoint        rv1        rv2  \n",
       "0       5.300000  13.275433  13.275433  \n",
       "1       5.200000  18.606195  18.606195  \n",
       "2       5.100000  28.642668  28.642668  \n",
       "3       5.000000  45.410389  45.410389  \n",
       "4       4.900000  10.084097  10.084097  \n",
       "...          ...        ...        ...  \n",
       "19730  13.333333  43.096812  43.096812  \n",
       "19731  13.300000  49.282940  49.282940  \n",
       "19732  13.266667  29.199117  29.199117  \n",
       "19733  13.233333   6.322784   6.322784  \n",
       "19734  13.200000  34.118851  34.118851  \n",
       "\n",
       "[19735 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"./KAG_energydata_complete.csv\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742a86b",
   "metadata": {},
   "source": [
    "## 2. Research Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9870756",
   "metadata": {},
   "source": [
    "A class to train models with different specifications in just one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c08ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCreator:\n",
    "  def __init__(self, weekday_enc='ohe', hour_enc='ohe', reg_type='lasso', poly_deg=1, random_state=None, trainer=None, split_rate=0.2):\n",
    "    self.params = {\n",
    "      'weekday_enc': weekday_enc,\n",
    "      'hour_enc': hour_enc,\n",
    "      'reg_type': reg_type,\n",
    "      'poly_deg': poly_deg,\n",
    "      'random_state': random_state,\n",
    "      'trainer': trainer,\n",
    "      'split_rate': split_rate\n",
    "    }\n",
    "    # self.model = None\n",
    "    # self.results = {}\n",
    "    self.encoders = {\n",
    "      'hour': None,\n",
    "      'weekday': None\n",
    "    }\n",
    "    self.ohe_counts = None\n",
    "    self.poly = None\n",
    "    \n",
    "  def __cyclic_encode(self, data, max_val):\n",
    "    return np.column_stack([\n",
    "        np.sin(2 * np.pi * data / max_val),\n",
    "        np.cos(2 * np.pi * data / max_val)\n",
    "    ])\n",
    "\n",
    "  def __ohe_encode(self, data, key):\n",
    "    if self.encoders[key] == None:\n",
    "      encoder_data = OneHotEncoder(sparse_output=False)\n",
    "      self.encoders[key] = encoder_data\n",
    "      encoder_data.fit(data.reshape(-1, 1))\n",
    "      # return (encoder_data.fit_transform(data.reshape(-1, 1)), encoder_data)\n",
    "    return (self.encoders[key].transform(data.reshape(-1, 1)), self.encoders[key])\n",
    "      \n",
    "\n",
    "  def __binned_encode(self, data):\n",
    "    pass\n",
    "    \n",
    "  def _process_dataframe(self, df):\n",
    "    # --- Handle The Dataframe ---\n",
    "    dataframe = df.copy()\n",
    "    date_column = pd.to_datetime(dataframe['date'])\n",
    "    hours = date_column.dt.hour.values # extracts hour column as [0, 0, ..., 2, 2, ...]\n",
    "    weekdays = date_column.dt.weekday.values # extracting weekdays like [0, 0, ..., 1, 1, ...]\n",
    "    dataframe = dataframe.drop('date', axis=1)\n",
    "    \n",
    "    # --- Handle Hour Encoding ---\n",
    "    if self.params['hour_enc'] == 'ohe':\n",
    "      hour_encoded, encoder_hour = self.__ohe_encode(hours, 'hour')\n",
    "      hour_column_names = encoder_hour.get_feature_names_out(['hour'])\n",
    "      self.encoder_hour = encoder_hour\n",
    "    elif self.params['hour_enc'] == 'trig':\n",
    "      hour_encoded = self.__cyclic_encode(hours, 24)\n",
    "      hour_column_names = np.array(['hour_sin', 'hour_cos'])\n",
    "    elif self.params['hour_enc'] == 'binned':\n",
    "      pass\n",
    "    else: raise Exception(\"this hour_enc is not supported or is not written correctly, please double check. supported hour_enc values: ohe, trig, binned\")\n",
    "    # put the hour-of-day into dataframe\n",
    "    hour_encoded_df = pd.DataFrame(hour_encoded, columns=hour_column_names)\n",
    "    dataframe = pd.concat([hour_encoded_df, dataframe], axis=1)\n",
    "    \n",
    "    # --- Handle Weekday Encoding ---   \n",
    "    if self.params['weekday_enc'] == 'ohe':\n",
    "      weekdays_encoded, encoder_weekdays = self.__ohe_encode(weekdays, 'weekday')\n",
    "      weekdays_column_names = encoder_weekdays.get_feature_names_out(['weekday'])\n",
    "    elif self.params['weekday_enc'] == 'trig':\n",
    "      weekdays_encoded = self.__cyclic_encode(weekdays, 7)\n",
    "      weekdays_column_names = np.array(['weekday_sin', 'weekday_cos'])\n",
    "    elif self.params['weekday_enc'] == 'binned':\n",
    "      pass\n",
    "    # Putting weekday columns into dataframe\n",
    "    week_encoded_df = pd.DataFrame(weekdays_encoded, columns=weekdays_column_names)\n",
    "    dataframe = pd.concat([week_encoded_df, dataframe], axis=1)\n",
    "    \n",
    "    # --- Handle Polynomial Features ---\n",
    "    if 'Appliances' in df:\n",
    "      X = dataframe.drop('Appliances', axis=1)\n",
    "      y = dataframe[['Appliances']]\n",
    "    else:\n",
    "      X = dataframe\n",
    "      y = None\n",
    "    \n",
    "    if self.params['poly_deg'] > 1:\n",
    "      if self.poly == None:\n",
    "        poly = PolynomialFeatures(degree=self.params['poly_deg'], include_bias=False)\n",
    "        X = poly.fit_transform(X)\n",
    "        self.poly = poly\n",
    "      else:\n",
    "        poly = self.poly\n",
    "        X = poly.transform(X)\n",
    "      # Filtering out bad ohe combinations\n",
    "      if self.ohe_counts == None:\n",
    "        ohe_counts = []\n",
    "        if self.params['weekday_enc'] == 'ohe':\n",
    "          if self.params['hour_enc'] == 'ohe':\n",
    "            ohe_counts = [(0, 7), (7, 7 + 24)]\n",
    "          elif self.params['hour_enc'] == 'binned':\n",
    "            pass\n",
    "          elif self.params['hour_enc'] == 'trig':\n",
    "            ohe_counts = [(0, 7)]\n",
    "        if self.params['weekday_enc'] == 'trig':\n",
    "          if self.params['hour_enc'] == 'ohe':\n",
    "            ohe_counts = [(2, 2 + 24)]\n",
    "          elif self.params['hour_enc'] == 'binned':\n",
    "            pass\n",
    "        if self.params['weekday_enc'] == 'binned':\n",
    "          if self.params['hour_enc'] == 'ohe':\n",
    "            pass\n",
    "          elif self.params['hour_enc'] == 'binned':\n",
    "            pass\n",
    "          elif self.params['hour_enc'] == 'trig':\n",
    "            pass\n",
    "        self.ohe_counts = ohe_counts\n",
    "        self.valid = []\n",
    "        if len(ohe_counts) > 0:\n",
    "          feature_powers = self.poly.powers_\n",
    "          for i in range(len(feature_powers)):\n",
    "            # if (feature_powers[i, ohe_counts[0][0]:ohe_counts[0][1]].sum() <= 1\n",
    "            #     and feature_powers[i, ohe_counts[1][0]:ohe_counts[1][1]].sum() <= 1): self.valid.append(i)\n",
    "            isValid = True\n",
    "            for item in ohe_counts:\n",
    "              if feature_powers[i, item[0]:item[1]].sum() > 1: isValid = False\n",
    "            if isValid: self.valid.append(i)\n",
    "      \n",
    "      if len(self.ohe_counts) > 0:\n",
    "        X = X[:, self.valid]\n",
    "    return (X, y)      \n",
    "\n",
    "  # def _extract_weekday(self, date_string: str) -> int:\n",
    "  #   return datetime.strptime(date_string, \"%Y-%m-%d\").weekday()\n",
    "\n",
    "  def fit(self, df):\n",
    "    \"\"\"Constructs the pipeline and trains it.\"\"\"\n",
    "    \n",
    "    X, y = self._process_dataframe(df)\n",
    "\n",
    "    # --- Split ---\n",
    "    if self.params['split_rate'] == 0: \n",
    "      # This means we don't want to split our data, we want the model to be trained on the whole dataframe.\n",
    "      # ATTENTION: You're allowed to use self.evaluate only when you've set the split_rate a positive number.\n",
    "      X_train = X\n",
    "      y_train = y\n",
    "    else:\n",
    "      # This means we want a test subset to be extracted from our dataframe\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.params['split_rate'], random_state=self.params['random_state'])\n",
    "      y_test = y_test.values.squeeze()\n",
    "      self.X_test = X_test\n",
    "      self.y_test = y_test\n",
    "    y_train = y_train.values.squeeze()\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "    \n",
    "    # --- PipeLine: Feature scaling & Training ---\n",
    "    if self.params['reg_type'] == 'lasso':\n",
    "      if self.params['trainer'] == None:\n",
    "        self.params['trainer'] = LassoCV(cv=5, random_state=self.params['random_state'], n_jobs=-1, max_iter=10000)\n",
    "      self.pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso', self.params['trainer'])\n",
    "      ])\n",
    "    elif self.params['reg_type'] == 'ridge':\n",
    "      if self.params['trainer'] == None:\n",
    "        self.params['trainer'] = RidgeCV()\n",
    "      self.pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', self.params['trainer'])        \n",
    "      ])\n",
    "    elif self.params['reg_type'] == 'regressor':\n",
    "      if self.params['trainer'] == None:\n",
    "        self.params['trainer'] = LinearRegression()\n",
    "      self.pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', self.params['trainer'])        \n",
    "      ])\n",
    "    else: raise Exception(\"this reg_type is not supported or is not written correctly, please double check. supported reg_type values: lasso, ridge, regressor\")\n",
    "  \n",
    "    self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, df, return_labels=False):\n",
    "    \"\"\"Gets raw dataframe and outputs model's predictions. Dataframe's structure should be exactly like the original dataframe, with the exception that including Appliance column is optional\"\"\"\n",
    "    X, y = self._process_dataframe(df)\n",
    "    y_pred = self.pipeline.predict(X)\n",
    "    if return_labels: return (y_pred, y)\n",
    "    return y_pred\n",
    "    \n",
    "  \n",
    "  # def evaluate(self, X_test, y_test):\n",
    "    # \"\"\"Calculates metrics and stores them in self.results.\"\"\"\n",
    "    # y_pred = self.model.predict(X_test)\n",
    "    # self.results['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    # self.results['r2'] = r2_score(y_test, y_pred)\n",
    "    # return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5da804",
   "metadata": {},
   "source": [
    "## 3. Training, Comparing And Analyzing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd942c9",
   "metadata": {},
   "source": [
    "### 3.1 How to Use The Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c20887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model = ModelCreator(hour_enc='ohe', weekday_enc='ohe', poly_deg=1, reg_type='regressor', random_state=42\n",
    "#                             , trainer=LinearRegression(random_state_24)) # Declare specifications\n",
    "# linear_model.fit(dataframe) # Train the model\n",
    "# y_pred = linear_model.pipeline.predict(linear_model.X_test) # Raise answers\n",
    "# RMSE = np.sqrt(mean_squared_error(linear_model.y_test, y_pred)) # calculate error\n",
    "# RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f110d",
   "metadata": {},
   "source": [
    "### 3.2 Choosing The Proper Encoding For Linear(degree=1) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8e8f9",
   "metadata": {},
   "source": [
    "#### 3.2.1 Trigonometric vs. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84ea95",
   "metadata": {},
   "source": [
    "Four models were trained differing only in encoding methods.\n",
    "\n",
    "The model with one-hot encoding for both variables has the highest ability of explaining the variance of the data(Highest $R^2$) along with lowest average prediction inaccuracy(lowest RMSE on both train and test). But it's obvious that all these models are underfitting, giving us inaccurate results, we can see this by comparing MAE(represents average distance between the model's predicted value and actual value) $MAE=50.88(Wh)$ to average(mean) energy consumption in our data $mean=97.69(Wh)$. The model's average innaccuracy is too high compared to average value of our target.\n",
    "\n",
    "All models have successfuly ignored the noise variables rv1 & rv2 by giving them a very low weight. This is not because of lasso's $\\lambda$, actually all these models have a $\\lambda$ that is effectively identical to $\\lambda=0$, we prove this in the next passage. The real reason why these models successfuly eliminated the noise variables, is because all of them are underfitting. The model gives high weights to noise variables when it's seeing patterns that don't exist, more formally, when it's overfitting(learning the noise instead of true patterns). When the model is underfitting, this does not happen.\n",
    "\n",
    "As a conclusion, models have the same performance on eliminating rv1 and rv2 and same test/train ratio, hence the ohe+ohe model is generally better in this table due to its better RMSE and $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91bb28",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE | rv1 | rv2 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 50.88 | 0.258 | 0.000 |\n",
    "| 1 | ohe | trig | lasso | 0.232 | 0.228 | 90.37 | 87.92 | 0.973 | 50.85 | 0.271 | 0.000 |\n",
    "| 1 | trig | ohe | lasso | 0.203 | 0.205 | 92.08 | 89.19 | 0.969 | 51.68 | -0.013 | -0.000 |\n",
    "| 1 | trig | trig | lasso | 0.200 | 0.201 | 92.26 | 89.40 | 0.969 | 51.62 | -0.001 | -0.000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e5bec",
   "metadata": {},
   "source": [
    "#### 3.2.2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5458b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, params):\n",
    "    # 2. Get Metrics\n",
    "    # (Using the evaluate method logic we discussed)\n",
    "    y_train_pred = model.pipeline.predict(model.X_train)\n",
    "    y_test_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    tr_r2 = r2_score(model.y_train, y_train_pred)\n",
    "    ts_r2 = r2_score(model.y_test, y_test_pred)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(model.y_train, y_train_pred))\n",
    "    ts_rmse = np.sqrt(mean_squared_error(model.y_test, y_test_pred))\n",
    "    \n",
    "    mae = mean_absolute_error(model.y_test, y_test_pred)\n",
    "    \n",
    "    rmse_ratio = ts_rmse / tr_rmse\n",
    "    \n",
    "    w_rv1 = model.pipeline.named_steps[params['reg_type']].coef_[-2]\n",
    "    w_rv2 = model.pipeline.named_steps[params['reg_type']].coef_[-1]\n",
    "    \n",
    "    # 3. Print Markdown Row\n",
    "    print(f\"| {params['poly_deg']} | {params['hour_enc']} | {params['weekday_enc']} | {params['reg_type']} | \"\n",
    "          f\"{tr_r2:.3f} | {ts_r2:.3f} | {tr_rmse:.2f} | {ts_rmse:.2f} | {rmse_ratio:.3f} | {mae:.2f} | {w_rv1:.3f} | {w_rv2:.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b20586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE | rv1 | rv2 |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 50.88 | 0.258 | 0.000 |\n",
      "| 1 | ohe | trig | lasso | 0.232 | 0.228 | 90.37 | 87.92 | 0.973 | 50.85 | 0.271 | 0.000 |\n",
      "| 1 | trig | ohe | lasso | 0.203 | 0.205 | 92.08 | 89.19 | 0.969 | 51.68 | -0.013 | -0.000 |\n",
      "| 1 | trig | trig | lasso | 0.200 | 0.201 | 92.26 | 89.40 | 0.969 | 51.62 | -0.001 | -0.000 |\n"
     ]
    }
   ],
   "source": [
    "# Define your experimental grid\n",
    "experiments = [\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "]\n",
    "\n",
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE | rv1 | rv2 |\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "\n",
    "for params in experiments:\n",
    "    # 1. Initialize and Fit\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe) # Assuming your raw data is in 'dataframe'\n",
    "    print_metrics(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114a380",
   "metadata": {},
   "source": [
    "### 3.3 Choosing The Proper Regularization For Linear(degree=1) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce836f",
   "metadata": {},
   "source": [
    "#### 3.3.1 Regularization Fails to Impact Underfitting Models, $\\lambda$ Becomes Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90706df",
   "metadata": {},
   "source": [
    "We concluded from the previous cell that OHE encoding is the best for our linear models. This time, four models with different regularization methods were tested and they all gave the same results. The reason to this is obvious, we talked earlier briefly in the ReadMe that $\\lambda$ tries to move the model in bias-variance tradeoff towards the sweetspot. Our linear model is underfitting, it has high bias and low variance, therefore the model tries to increase the model's variance to move it towards the sweetspot by decreasing $\\lambda$, but it reaches the model's boundries, meaning $\\lambda$ gets near zero and tries it best to increase the variance, but the model has reached its limits and it can't go further(can't have more variance). Hence, all models, regardless of their regularization method, reach the same spot.\n",
    "\n",
    "In another words, even when $\\lambda = 0$(normal regression without regularization) the model is underfitting, meaning it has high bias and what it needs is more variance, and any $\\lambda > 0$ provides less variance, hence, the model tries to push $\\lambda$ towards zero, pushing all regulrazied regressions towards becoming a normal regression without regularization, therefore all four models end up being the same(not excatly the same, but almost identical).\n",
    "\n",
    "Considering the observations above, we conclude that when our model is underfitting, there's nothing regularization L1 or L2 can do, but to give us the same model we had. Utilizing regularization methods like L1 and L2 when the model is underfitting doesn't provide any value.\n",
    "\n",
    "Regularization is meant to reduce model's complexity to decrease variance, using it when the model is too simple and needs more variance doesn't help, therefore, the best linear model with degree one is a regular linear regression model with one-hot encoded weekdays and hours but without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b98f62",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Best $\\lambda$ | rv1 | rv2 | CrossVal |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 | 0.258 | 0.000 | 5 Folds |\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 0.139 | 0.139 | LOOCV |\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 0.139 | 0.139 | 5 Folds |\n",
    "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | - | 0.142 | 0.142 | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff77db7",
   "metadata": {},
   "source": [
    "#### 3.3.2 Numerical Stalemate: Why Didn't Ridge $\\lambda$ Become Zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a6236",
   "metadata": {},
   "source": [
    "But you might ask, if L1 and L2 try to push the $\\lambda$ to zero for this model, why does this happens for lasso($\\lambda = 0.0197$) but not for ridge($\\lambda = 11.4976$)? Even when we try to train a ridge model with alphas=[0.0197, 11.4976], we expect the model to pick the less lambda based on the logic we provided above to increase the model's variance, but it picks 11.4976(check the code below to see), does this mean our logic is wrong? Well, not percisely.\n",
    "\n",
    "While it seems that ridge is working against this logic, this is not actually true. I trained two ridge models, one with $\\lambda = 0.0197$ and another with $\\lambda = 11.4976$, By looking at the table below, they both raised the same results! This means both $\\lambda$ values give our model almost the same variance, they both create the same model. But why?\n",
    "\n",
    "The reason comes from how Ridge treats our data. Lasso is much more aggressive in penalizing variance comparing to Ridge, hence it's much more sensitive to changing the value of $\\lambda$. As shown in the table below, the same change for $\\lambda$ effects Lasso regression remarkably while Ridge regression stays almost the same.\n",
    "\n",
    "But if ridge regression with $\\lambda = 0.0197$ and $\\lambda = 11.4976$ are almost identical, why doesn't the model choose the lower lambda? that lower lambda might have the same performance but just be a little bit better, but this amount is so small the model can't see it properly.\n",
    "\n",
    "Our data is noisy, and K-Fold cross validation isn't perfect. Lasso with $\\lambda = 0.0197$ might be $0.00001$ better in performance, but the noise and very little uncontrolable bias that can happen in K-Fold CV, can hide this fact, making the model think $\\lambda = 11.4976$ is an option that's a little bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5a529",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Fixed $\\lambda$ | rv1 | rv2 | CrossVal |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 0.0197 | 0.142 | 0.142 | 5 Folds |\n",
    "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 0.139 | 0.139 | 5 Folds |\n",
    "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 | 0.258 | 0.000 | 5 Folds |\n",
    "| 1 | ohe | ohe | lasso | 0.056 | 0.060 | 100.22 | 97.01 | 0.968 | 11.4976 | -0.000 | -0.000 | 5 Folds |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d324a",
   "metadata": {},
   "source": [
    "#### 3.3.3 Bias-Variance Tradeoff - Visual Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2eeba",
   "metadata": {},
   "source": [
    "Here's a visual summary:\n",
    "\n",
    "<img src=\"./visuals/diagram-bias-variance-tradeoff.png\" alt=\"bias variance tradeoff explained with diagram\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ec3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Best $\\lambda$ | rv1 | rv2 |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 | 0.258 | 0.000 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 0.139 | 0.139 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 0.139 | 0.139 |\n",
      "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | -1.0000 | 0.142 | 0.142 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 0.0197 | 0.142 | 0.142 |\n",
      "| 1 | ohe | ohe | ridge | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 11.4976 | 0.139 | 0.139 |\n",
      "| 1 | ohe | ohe | lasso | 0.235 | 0.232 | 90.19 | 87.68 | 0.972 | 0.0197 | 0.258 | 0.000 |\n",
      "| 1 | ohe | ohe | lasso | 0.056 | 0.060 | 100.22 | 97.01 | 0.968 | 11.4976 | -0.000 | -0.000 |\n"
     ]
    }
   ],
   "source": [
    "# Define your experimental grid\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "experiments = [\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'regressor', 'trainer': LinearRegression()},\n",
    "    # {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=[0.0197, 11.4976])},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=[0.0197])},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=[11.4976])},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, alphas=[0.0197], random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, alphas=[11.4976], random_state=24, n_jobs=-1, max_iter=10000)},\n",
    "]\n",
    "\n",
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Best $\\\\lambda$ | rv1 | rv2 |\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "\n",
    "for params in experiments:\n",
    "    # 1. Initialize and Fit\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe) # Assuming your raw data is in 'dataframe'\n",
    "    \n",
    "    # 2. Get Metrics\n",
    "    # (Using the evaluate method logic we discussed)\n",
    "    y_train_pred = model.pipeline.predict(model.X_train)\n",
    "    y_test_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    tr_r2 = r2_score(model.y_train, y_train_pred)\n",
    "    ts_r2 = r2_score(model.y_test, y_test_pred)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(model.y_train, y_train_pred))\n",
    "    ts_rmse = np.sqrt(mean_squared_error(model.y_test, y_test_pred))\n",
    "    \n",
    "    rmse_ratio = ts_rmse / tr_rmse\n",
    "    \n",
    "    w_rv1 = model.pipeline.named_steps[params['reg_type']].coef_[-2]\n",
    "    w_rv2 = model.pipeline.named_steps[params['reg_type']].coef_[-1]\n",
    "    \n",
    "    try:\n",
    "        alpha = model.pipeline.named_steps[params['reg_type']].alpha_\n",
    "    except AttributeError: alpha = -1\n",
    "    \n",
    "    # 3. Print Markdown Row\n",
    "    print(f\"| {params['poly_deg']} | {params['hour_enc']} | {params['weekday_enc']} | {params['reg_type']} | \"\n",
    "          f\"{tr_r2:.3f} | {ts_r2:.3f} | {tr_rmse:.2f} | {ts_rmse:.2f} | {rmse_ratio:.3f} | {alpha:.4f} | {w_rv1:.3f} | {w_rv2:.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057c3c9",
   "metadata": {},
   "source": [
    "#### 3.3.4 Final Look: Experiment Confirms Our Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795d7f0",
   "metadata": {},
   "source": [
    "We saw that one-hot encoding worked best for lasso. Now that we have chosen normal regression without regularization, lets see if this is also true for this model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aeebea",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE | rv1 | rv2 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 50.92 | 0.142 | 0.142 |\n",
    "| 1 | ohe | trig | regressor | 0.232 | 0.227 | 90.37 | 87.93 | 0.973 | 50.89 | 0.149 | 0.149 |\n",
    "| 1 | trig | ohe | regressor | 0.203 | 0.205 | 92.08 | 89.20 | 0.969 | 51.71 | -0.022 | -0.022 |\n",
    "| 1 | trig | trig | regressor | 0.200 | 0.201 | 92.25 | 89.42 | 0.969 | 51.65 | -0.018 | -0.018 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776142e",
   "metadata": {},
   "source": [
    "As expected, we got the same table we got for lasso. This is just an experimental proof for all the logic and theory we discussed earlier. Regression with lasso, with ridge, or with neither, they all give the same performance, because the model is underfitting and lacking variance. This is why we got the same table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ca93c",
   "metadata": {},
   "source": [
    "#### 3.3.5 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f633772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE | rv1 | rv2 |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 50.92 | 0.142 | 0.142 |\n",
      "| 1 | ohe | trig | regressor | 0.232 | 0.227 | 90.37 | 87.93 | 0.973 | 50.89 | 0.149 | 0.149 |\n",
      "| 1 | trig | ohe | regressor | 0.203 | 0.205 | 92.08 | 89.20 | 0.969 | 51.71 | -0.022 | -0.022 |\n",
      "| 1 | trig | trig | regressor | 0.200 | 0.201 | 92.25 | 89.42 | 0.969 | 51.65 | -0.018 | -0.018 |\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'regressor', 'trainer': LinearRegression()},\n",
    "    {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'regressor', 'trainer': LinearRegression()},\n",
    "    {'poly_deg': 1, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'regressor', 'trainer': LinearRegression()},\n",
    "    {'poly_deg': 1, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'regressor', 'trainer': LinearRegression()},\n",
    "]\n",
    "\n",
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Test MAE | rv1 | rv2 |\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "\n",
    "for params in experiments:\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe)\n",
    "    print_metrics(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272dc6e9",
   "metadata": {},
   "source": [
    "### 3.4 Choosing The Best Polynomial Model(degree two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a5972",
   "metadata": {},
   "source": [
    "#### 3.4.1 Why to Use Degree=2 Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af0176",
   "metadata": {},
   "source": [
    "As we saw, linear models with degree one could not explain our data properly. They were underfitting, lacking variance. Therefore, we increase the complexity of our model by changing its polynomial degree to two, to get a model with higher variance that has more chance to learn our data, due to its flexiblity to learn new patterns and less bias on the patterns it already knows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cfb6f",
   "metadata": {},
   "source": [
    "#### 3.4.2 How I Tuned LassoCV to Converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e063a45",
   "metadata": {},
   "source": [
    "When training a lasso regression with one-hot encoded columns, I got a 'ConvergenceWarning' from scikit-learn. The data had over 1400 features with high multicollinearity due to a lot of columns representing same information or representing information that can be extracted from other columns. These multicollinearities happen often with noisy datasets like the one we are working with right now.\n",
    "\n",
    "Becuase the data had a lot of redunant features, the loss function plot's shape had become nearly flat around the optimal point. Lasso takes steps in a path towards that optimal point, and when it gets close, the shape becomes flat meaning lasso will take smaller and smaller steps, when we get to a point that we have taken lots of steps but we haven't reached the optimal point yet. This was the reason why we got the 'ConvergenceWarning' warning.\n",
    "\n",
    "To fix this problem, the hyperparmeters tol and max_iter needed to be tuned properly. max_iter is the maximum number of steps our lasso model takes to reach the optimal point. And to know about tol, first we have to know what is $Duality Gap$.\n",
    "\n",
    "Computations aren't perfect. Computer can not always reach the exact optimal spot, but it can get very close to it, by taking steps in a path towards the point and stopping somewhere near the optimal point. To determine when we should stop, we define duality gap. Duality gap is the difference of the theoretical possible minimum for our cost function and the current value of our cost function. The theoretical minimums is calculated using mathematic relations. We can think of duality gap as the distance is left to reach the optimal point. Hence, we define tol. We tell the model that if your distance from the optimal point(duality gap) is less than the value of tol, you are close enough(convergence has happened), you are good to stop computations and consider the weights of the current spot as an answer.\n",
    "\n",
    "$$\\text{Duality Gap} = \\text{Current Cost (Primal)} - \\text{Theoretical Floor (Dual)}$$\n",
    "\n",
    "The problem of our model was that it would stop in a spot that was not close enough to the optimal spot($Duality Gap > tol$) because it had reached the maximum number of steps it could take(max_iter). This means that convergence didn't happen, resulting in a 'ConvergenceWarning'. The model had finished the training and all the process without error, but its answer is not accurate and 'good enough'. Therefore, by increasing the value of tol and max_iter, we can make sure convergence happens.\n",
    "\n",
    "The cost function used in LassoCV is:\n",
    "\n",
    "$$J(w) = \\frac{1}{2n_{\\text{samples}}} \\|y - Xw\\|_2^2 + \\alpha \\|w\\|_1$$\n",
    "\n",
    "The second term is L1 norm, the first term is actually $\\frac{1}{2} MSE$. It's clear that $2 \\times J(w) > MSE$ and $J_{\\text{answer}}(w) - J_{\\text{theoretical minimum}}(w) < tol$ and $MAE < MSE$. This means the difference of our final $MAE$ and the smallest possible $MAE$ is guaranteed to be smaller than two times the value of our $tol$. This logic helps us to increase $tol$ with open eyes without making the model too much inaccurate.\n",
    "\n",
    "$MAE$ shows the average innaccuracy of our model in predicting the target variable. Our target variable has an average(mean) value of 97.69(Wh). Hence, if the average error of our model is 0.05(Wh) more than the best $MAE$, it's still good, hence $tol=0.1$ seems like a good number. The previous $tol=10^{-4}$ was an overkill for this dataset.\n",
    "\n",
    "This increase in $tol$ means the model can reach a spot with more distance from the optimal value than before, but still be considered as a convergence. This makes the destination more accessible, increasing the probablity that convergence happens in a fair amount of steps. But still, we have to increase max_iter to let the model be able to take more steps, again increasing convergence probablity. $40000$ seems like a good number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d42ce6",
   "metadata": {},
   "source": [
    "#### 3.4.3 Training The Models (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f097a6",
   "metadata": {},
   "source": [
    "let's train the models and compare them at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5843947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, params):\n",
    "    y_train_pred = model.pipeline.predict(model.X_train)\n",
    "    y_test_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    tr_r2 = r2_score(model.y_train, y_train_pred)\n",
    "    ts_r2 = r2_score(model.y_test, y_test_pred)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(model.y_train, y_train_pred))\n",
    "    ts_rmse = np.sqrt(mean_squared_error(model.y_test, y_test_pred))\n",
    "    \n",
    "    rmse_ratio = ts_rmse / tr_rmse\n",
    "    r2_ratio = ts_r2 / tr_r2\n",
    "    \n",
    "    w_rv1 = model.pipeline.named_steps[params['reg_type']].coef_[-2]\n",
    "    w_rv2 = model.pipeline.named_steps[params['reg_type']].coef_[-1]\n",
    "    \n",
    "    try:\n",
    "        alpha = model.pipeline.named_steps[params['reg_type']].alpha_\n",
    "    except AttributeError: alpha = -1    \n",
    "    \n",
    "    print(f\"| {params['poly_deg']} | {params['hour_enc']} | {params['weekday_enc']} | {params['reg_type']} | \"\n",
    "          f\"{tr_r2:.3f} | {ts_r2:.3f} | {tr_rmse:.2f} | {ts_rmse:.2f} | {rmse_ratio:.3f} | {r2_ratio:.3f} | {alpha:.4f} | {w_rv1:.3f} | {w_rv2:.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d58425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelCreator at 0x7a19e81ba8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your experimental grid\n",
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "]\n",
    "lasso_ohe_ohe = ModelCreator(**experiments[0], random_state=42)\n",
    "lasso_ohe_ohe.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06b9b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\lambda$ | rv1 | rv2\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| 2 | ohe | ohe | lasso | 0.392 | 0.333 | 80.45 | 81.70 | 1.016 | 0.850 | 0.0201 | -0.0 | -0.0 |\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "]\n",
    "print(\"| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\\\lambda$ | rv1 | rv2\")\n",
    "print(\"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "print_metrics(lasso_ohe_ohe, experiments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77c68f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelCreator at 0x7a19e81b8b60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "]\n",
    "lasso_ohe_trig = ModelCreator(**experiments[0], random_state=42)\n",
    "lasso_ohe_trig.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelCreator at 0x7a1a0e9e8e30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "]\n",
    "lasso_trig_ohe = ModelCreator(**experiments[0], random_state=42)\n",
    "lasso_trig_ohe.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58c954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelCreator at 0x7a19e81b8ad0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "]\n",
    "lasso_trig_trig = ModelCreator(**experiments[0], random_state=42)\n",
    "lasso_trig_trig.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a0d03d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2 | ohe | trig | lasso | 0.368 | 0.322 | 82.02 | 82.39 | 1.005 | 0.875 | 0.0201 | -0.0 | -0.0 |\n",
      "| 2 | trig | ohe | lasso | 0.324 | 0.296 | 84.78 | 83.91 | 0.990 | 0.914 | 0.0251 | -0.0 | -0.0 |\n",
      "| 2 | trig | trig | lasso | 0.302 | 0.284 | 86.17 | 84.63 | 0.982 | 0.941 | 0.0251 | -7.920922314021879e-16 | -7.920922314021879e-16 |\n"
     ]
    }
   ],
   "source": [
    "experiments = [{'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "               {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)},\n",
    "               {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)}\n",
    "               ]\n",
    "print_metrics(lasso_ohe_trig, experiments[0])\n",
    "print_metrics(lasso_trig_ohe, experiments[1])\n",
    "print_metrics(lasso_trig_trig, experiments[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bdef94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2 | ohe | ohe | ridge | 0.473 | 0.347 | 74.86 | 80.84 | 1.080 | 0.733 | 0.0071 | -0.08808101384147449 | -0.08808101384147449 |\n",
      "| 2 | ohe | trig | ridge | 0.445 | 0.343 | 76.83 | 81.08 | 1.055 | 0.771 | 0.0040 | -0.045100273189746076 | -0.045100273189746076 |\n",
      "| 2 | trig | ohe | ridge | 0.403 | 0.331 | 79.69 | 81.80 | 1.027 | 0.822 | 0.0035 | -0.04664192794371047 | -0.04664193069402245 |\n",
      "| 2 | trig | trig | ridge | 0.374 | 0.324 | 81.60 | 82.24 | 1.008 | 0.867 | 0.0023 | -0.01834038327433518 | -0.01834038327433518 |\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)}\n",
    "]\n",
    "\n",
    "for params in experiments:\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe)\n",
    "    print_metrics(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d62a6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2 | ohe | ohe | ridge | 0.472 | 0.347 | 74.97 | 80.86 | 1.079 | 0.735 | 0.0142 |\n",
      "| 2 | ohe | trig | ridge | 0.444 | 0.343 | 76.90 | 81.09 | 1.055 | 0.772 | 0.0071 |\n",
      "| 2 | trig | ohe | ridge | 0.402 | 0.331 | 79.73 | 81.80 | 1.026 | 0.823 | 0.0053 |\n",
      "| 2 | trig | trig | ridge | 0.373 | 0.324 | 81.64 | 82.24 | 1.007 | 0.868 | 0.0040 |\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 3, 100)\n",
    "\n",
    "experiments = [\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'ohe', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)},\n",
    "    {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=5, alphas=alphas)}\n",
    "]\n",
    "\n",
    "for params in experiments:\n",
    "    model = ModelCreator(**params, random_state=42)\n",
    "    model.fit(dataframe)\n",
    "    print_metrics(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e474f",
   "metadata": {},
   "source": [
    "#### 3.4.4 Analyzing Degree=2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765a0d0",
   "metadata": {},
   "source": [
    "##### Table-1 Lasso Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78746a",
   "metadata": {},
   "source": [
    "RMSE Ratios are almost identical, but $R^2$ ratios differ remarkably. Due to better Ratio, models with trig encoding are more stable but less accurate, due to higher RMSE and lower $R^2$. While the ohe+ohe model has less stability, it still stable enough $R^2_{test}/R^2_{train} = 0.85$. Therefore we choose the ohe+ohe model for its better performance(RMSE & $R^2$) over the test set.\n",
    "\n",
    "By looking at the training time, we can see the theory we discussed earlier in the ReadMe. Models with one-hot encoding have a slower training as it is obvious in the table, due to high number of columns they create. All models are clearly underfitting, due to Train RMSE & Test RMSE being almost indentical(close to one RMSE ratio, very stable), but both RMSE metrics are very high compared to the mean of the target value. This underfitting is the reason why $\\lambda$ is very close to zero again. All models have also successfully handled the random noise, again confirming that the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64384afe",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\lambda$ | rv1 | rv2 | CrossVal | Training Time |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 2 | ohe | ohe | lasso | 0.392 | 0.333 | 80.45 | 81.70 | 1.016 | 0.850 | 0.0201 | -0.0 | -0.0 | 5-Folds | 295.5(s) |\n",
    "| 2 | ohe | trig | lasso | 0.368 | 0.322 | 82.02 | 82.39 | 1.005 | 0.875 | 0.0201 | -0.0 | -0.0 | 5-Folds | 260.8(s) |\n",
    "| 2 | trig | ohe | lasso | 0.324 | 0.296 | 84.78 | 83.91 | 0.990 | 0.914 | 0.0251 | -0.0 | -0.0 | 5-Folds | 161.4(s) |\n",
    "| 2 | trig | trig | lasso | 0.302 | 0.284 | 86.17 | 84.63 | 0.982 | 0.941 | 0.0251 | -7.92e-16 | -7.92e-16 | 5-Folds | 153.6(s) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca8975",
   "metadata": {},
   "source": [
    "##### Table-2: Ridge Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1d574",
   "metadata": {},
   "source": [
    "By replacing trigonometric encoding with one-hot encoding, Test $R^2$ stays the same, the models ability to explain the test set does not change, but the Train $R^2$ increases. This increase means the model is learning more, because it can explain more of the training set, but what it is learning is noise, not patterns, therefore the model can explain the training data better but its ability to explain unseen data remains the same. we can see the same pattern for RMSE. In another words, replacing trig encoding with one-hot pushes the model towards overfitting. The lower $R^2$ ratio in ridge+ohe models confirms this fact.\n",
    "\n",
    "In short, all models have almost the same performance on the test set but the fourth model(trig+trig+ridge) has better generalization and is more stable, due to its better ratios. hence it's the best ridge model out of this list. It's also more efficient due to less amount of features it creates.\n",
    "\n",
    "The weights chosen for random variables also acknowledge the claim. while all models successfully determined these variables are irrelevant to our target hence minimized their weights, the trig+trig model has gave them weights that are four times less than the weights in ohe+ohe. This means that ohe+ohe model is more hallucinating compared to the fourth model, confirming the overfitting claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdbe80",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\lambda$ | rv1 | rv2 | CrossVal |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 2 | ohe | ohe | ridge | 0.473 | 0.347 | 74.86 | 80.84 | 1.080 | 0.733 | 0.0071 | -0.088 | -0.088 | LOOCV |\n",
    "| 2 | ohe | trig | ridge | 0.445 | 0.343 | 76.83 | 81.08 | 1.055 | 0.771 | 0.0040 | -0.045 | -0.045 | LOOCV |\n",
    "| 2 | trig | ohe | ridge | 0.403 | 0.331 | 79.69 | 81.80 | 1.027 | 0.822 | 0.0035 | -0.046 | -0.046 | LOOCV |\n",
    "| 2 | trig | trig | ridge | 0.374 | 0.324 | 81.60 | 82.24 | 1.008 | 0.867 | 0.0023 | -0.018 | -0.018 | LOOCV |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813de881",
   "metadata": {},
   "source": [
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | best $\\lambda$ | CrossVal |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 2 | ohe | ohe | ridge | 0.472 | 0.347 | 74.97 | 80.86 | 1.079 | 0.735 | 0.0142 | 5 Folds |\n",
    "| 2 | ohe | trig | ridge | 0.444 | 0.343 | 76.90 | 81.09 | 1.055 | 0.772 | 0.0071 | 5 Folds |\n",
    "| 2 | trig | ohe | ridge | 0.402 | 0.331 | 79.73 | 81.80 | 1.026 | 0.823 | 0.0053 | 5 Folds |\n",
    "| 2 | trig | trig | ridge | 0.373 | 0.324 | 81.64 | 82.24 | 1.007 | 0.868 | 0.0040 | 5 Folds |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc925fa",
   "metadata": {},
   "source": [
    "### 3.5 Why We Should not Go For Higher Degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d5110",
   "metadata": {},
   "source": [
    "We increased the degree of our model to 2, but still, the model has a very high error $RMSE=80(Wh)$ compared to average value of our dataset $mean=97.69(Wh)$. This might lead us to consider training more complex models(degree > 3) to decrease this error, but that is not possible.\n",
    "\n",
    "Our dataset has around 20,000 rows, and our models have 32-59 columns based on their encoding method. The number of features after applying a degree $d$ polynomial features on a dataset with $P$ columns is:\n",
    "\n",
    "$$\\binom{P + d}{d} = \\frac{(P + d)!}{P! d!}$$\n",
    "\n",
    "This means we will have:\n",
    "\n",
    "| Columns ($n$) | Degree 2 Features | Degree 3 Features | Degree 4 Features |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| **32** | 561 | 6,545 | 52,360 |\n",
    "| **59** | 1,830 | 37,820 | 595,665 |\n",
    "\n",
    "To prevent overfitting, it is suggested to have an $N/P$ ratio over 10 in normal cases and 20 for noisy datasets, where $N$ is the number of samples(rows) and $P$ is the number of features(columns). To train a degree three model without the risk of overfitting, we need at least 60,000 rows, while we only have 20,000. Hence, we don't go for higher degrees.\n",
    "\n",
    "You might say that the data filtering we implemented to eliminate OHE-OHE columns will decrease the number of features, but even with data filtering, the number of features in degree three is much more than the boundary our number of rows can handle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d67cc6",
   "metadata": {},
   "source": [
    "## 4. Final Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a042d9",
   "metadata": {},
   "source": [
    "#### 4.1 Introducing The Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a355c",
   "metadata": {},
   "source": [
    "So far, we have selected three models out of 16 models, one linear, one polynomial with L1 and another polynomial with L2. Now it's time to compare, plot, and explain. \n",
    "\n",
    "Lasso model has a slightly(~1%) better $RMSE$ & $R^2$ comparing to the ridge model but it's 30 times slower in terms of convergence speed. Also considering they are almost the same in other metrics, ridge is preferred over lasso in this table. This leaves us with degree two ridge and degree one normal regression (no regularization). Both have good acceptable ratios for $R^2$ and $RMSE$, but ratios for the normal regression is better, signaling more stability. Ridge has identified noise variables better and gave them less weights comparing to degree one model. It also makes more accurate predictions, the better $R^2$ and $RMSE$ values acknowledge this fact, this covers the worse ratios it has.\n",
    "\n",
    "If we aim for a balance in accuracy and stability, Ridge has good enough ratios (but not perfect) with more accurate predictions, hence, **the ridge model with trigonomteric encoding seems to be the best out of all the models we have trained.**. The degree one regression model might be nearly perfect in ratios, but it's remarkably less accurate.\n",
    "\n",
    "All the models we have trained so far, despite all the improvments, are underfitting. This can be seen by looking at RMSE. All models have a high RMSE on train and test compared to the mean of the target variable. But why? we did regularizations, encodings, why didn't we reach a practical model we can use in real world? The reason to this is **noise**.\n",
    "\n",
    "The target variable we're predicting depends on many factors that do not exist in the given dataset. This is where we say *our data is noisy*. Energy consumption is effected by a lot of sudden and random spikes called noise. Someone might decide to turn on conditioner for thirty minutes in 3AM 12 July because he woke up due to nightmare, and this incident never happened again. These spikes are very hard to explain by mathematic relations or formulas like regression. These spikes happen in every dataset, that is why we always have errors in regression models, But the amount of these spikes in this dataset is much more compared to others, that is why we say this data is noisy.\n",
    "\n",
    "Another way to identify this noise is to look for the different target values we have when the values of our dependent variables are almost the same. This means our target is effected by some other variable that is not listed in our dataset.\n",
    "\n",
    "Enough talking, lets get to plots. We didn't find a unicorn perfect model for sure, but we can analyze, plot and explain the current models to discover new interesting information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bd3d9",
   "metadata": {},
   "source": [
    "##### Currently Selected Models\n",
    "\n",
    "| Deg | Hour | Week | Reg | Train R² | Test R² | Train RMSE | Test RMSE | Ratio (Test/Train RMSE) | Ratio($R^2$) | Best $\\lambda$ | rv1 | rv2 | CrossVal | Training Time |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 0.987 | - | 0.142 | 0.142 | - | Less than 1(s) |\n",
    "| 2 | ohe | ohe | lasso | 0.392 | 0.333 | 80.45 | 81.70 | 1.016 | 0.850 | 0.0201 | -0.0 | -0.0 | 5-Folds | 295.5(s) |\n",
    "| 2 | trig | trig | ridge | 0.374 | 0.324 | 81.60 | 82.24 | 1.008 | 0.867 | 0.0023 | -0.018 | -0.018 | LOOCV | Less than 10(s) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8a6c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | ohe | ohe | regressor | 0.235 | 0.232 | 90.19 | 87.69 | 0.972 | 0.985 | -1.0000 | 0.142 | 0.142 |\n"
     ]
    }
   ],
   "source": [
    "params = {'poly_deg': 1, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'regressor', 'trainer': LinearRegression()}\n",
    "regressor = ModelCreator(**params, random_state=42)\n",
    "regressor.fit(dataframe)\n",
    "print_metrics(regressor, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a073b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2 | ohe | ohe | lasso | 0.392 | 0.333 | 80.45 | 81.70 | 1.016 | 0.850 | 0.0201 | -0.000 | -0.000 |\n"
     ]
    }
   ],
   "source": [
    "params = {'poly_deg': 2, 'hour_enc': 'ohe', 'weekday_enc': 'ohe', 'reg_type': 'lasso', 'trainer': LassoCV(cv=5, random_state=24, tol=0.1, n_jobs=-1, max_iter=40000)}\n",
    "lasso = ModelCreator(**params, random_state=42)\n",
    "lasso.fit(dataframe)\n",
    "print_metrics(lasso, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6cdf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2 | trig | trig | ridge | 0.374 | 0.324 | 81.60 | 82.24 | 1.008 | 0.867 | 0.0023 | -0.018 | -0.018 |\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 3, 100)\n",
    "params = {'poly_deg': 2, 'hour_enc': 'trig', 'weekday_enc': 'trig', 'reg_type': 'ridge', 'trainer': RidgeCV(cv=None, alphas=alphas)}\n",
    "ridge = ModelCreator(**params, random_state=42)\n",
    "ridge.fit(dataframe)\n",
    "print_metrics(ridge, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f7a2b",
   "metadata": {},
   "source": [
    "### 4.2 Deeper Dive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
